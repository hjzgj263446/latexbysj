\chapter{总结与展望}
\section{本文工作总结}
随着互联网的飞速发展，网络中充斥着大量的文本数据。例如网站论坛上包含了丰富类别的文本数据，如娱乐类信息，政治类新闻，人物描述等。这些文本数据的产生必然伴随着对数据的归类，如何提升分类
效率，减少人工成本，这便是文本数据分类的研究方向。此外在美团、大众点评等网站上具有用户发表的含有感情色彩的评论。从这些海量数据中挖掘出用户的情感，有助于精准地刻画用户，从而辅助平台进行针对性的提供服务。
同时，用户对某一事项的不同方面进行评价，有利于针对这些方面的不足之处进行改进以提升用户感知。方面级情感分类研究便呼之而出。传统的机器学习方法需要大量的手工提取特征的方式，增加了研究者的负担。而一些采用深度学习的方式在文本处理领域略有不足，
例如很多算法忽略了文本的整体信息以及文本在语料库中的统计信息。尤其是在对于方面级目标词与文本内容之间关系的捕捉之上，很多模型都具有改进空间。

基于上述问题，本文提出了一个基于图卷积神经网络的整体文本分类算法和方面级情感分析算法。整体文本分类算法即是对一段文本所属的类别进行归纳，一段文本对应一个标签。方面级
情感分析任务关注于文本中的部分词的情感信息，即一段文本根据目标词汇的不同，可能具有多种情感标签。本文提出的方法采用图模型将单词之间建立联系，并通过这些联系深层次的挖掘
文本语义信息，进而提升模型分类性能。以下是本文的工作内容总结：

（1）对于整体文本分类任务而言，关注的是整体句子的含义。本文提出一种滑动窗口构图的方式，将文本中的不同单词之间建立联系。此外引入了一个超节点，用以连接文本中所有单词，而超节点与
其他节点的边的信息依赖于语料库中文本与单词之间的联系。超节点的创建用以捕获文本整体信息。通过图模型的信息传递机制，将单词节点之间的信息进行交互，并通过一个注意力机制确保节点关注于重要信息。
经过图卷积神经网络的学习后，每个单词节点获得了更加丰富的向量表示，节点之间包含了上下文信息，而超节点通过图模型也获得了整个文本内容的信息。将单词向量构成文本矩阵，再采用CNN网络提取文本中的
关键信息，最后与超节点代表的文本整体信息进行结合，实现文本分类任务。从实验结果来看，本文提出的基于图卷积的整体文本分类算法在准确率和F1分数上均高于对比方法。准确率上在MR数据集上高于第二优的Bi-LSTM模型
1.64个百分点，并远高于MLP7.97\%。对比与同样采用了CNN结构的TextCNN模型来说，准确率和F1分数分别在R52数据集上高出1.43\%和11.18\%，即使是未使用超节点向量的TextGraph-CNN，在R52数据集上准确率高出TextCNN模型
0.8\%，F1分数高出7.27\%，说明本文提出的采用图网络方式学习到的单词向量表示更具有意义。此外，该算法在低比例训练集下展现了不错的分类效果，这种结论与同样采用GCN结构的方法TextGCN类似。超节点向量
和CNN结构提取出的关键信息向量，从两个角度描述了文本信息，因此，从结果来看，两者的结合丰富了文本向量表示，提升了模型分类性能。

（2）对于方面级情感分类任务而言，关注的是当前方面词在文本中的情感色彩。本文依然采用滑动窗口的方式构建文本单词图。不同于整体文本分类算法，超节点不再与所有单词节点建立联系，而仅仅与
方面词建立连接。通过超节点的建立，将方面词中的各个单词融合起来，通过图模型将方面词整体信息传递给方面词中的每个单词，这样促使方面词中各个单词实现紧密联系。此外，随着网络中节点的向更远处
节点的信息传递，有助于将超节点信息传递给上下文中的其他单词，实现方面词与上下文之间的联系。本文还在图卷积神经网络中使用了注意力机制以及门控机制，辅助模型控制信息流动。对于重要的邻居节点
则从中获取更多的信息，对于不重要的节点则选择从中获得少量的信息。从实验结果来看，本文提出的算法在准确率和F1分数上高于绝大多数对比方法。如准确率在Twitter数据集上高于SVM接近9\%，比同样
采用了GCN结构的ASGCN-DT和DGEDT(BiGCN)分别高于1.07\%和0.4\%。尤其是对比未使用注意力机制和门控机制的GCN来说，所有数据集上的表现明显更优，例如在Rest15数据集上准确率和F1分数分别高出3.14\%，
7.87\%。证明本文提出的注意力机制和门控机制的有效性。另外，本文还结合了BERT预训练模型进一步提升模型性能。相比于同样采用了BERT模型的TG-BERT明显更好，如在Rest14数据集上准确率高出1.46\%。
和目前最优的模型之一DGEDT-BERT相比，各有优势。在Twitter数据集上本文的方法略低，但是同样在Rest15数据集上高出1.23\%。同时本文提出的方法相比DGEDT-BERT少了一个计算量较大的transformer结构，在
低计算资源情况下更具优势。最后，本文还提出了一个数据增扩的方式，实验结果表明，能够提升模型分类性能，如在Rest14数据集上提升了1.56\%。

\section{未来工作展望}
文本分类任务是自然语言处理中的关键任务之一。本文提出了基于图模型的文本分类算法，根据具体任务的不同，超节点构建方式具有差异。虽然在实验数据集上展示不错的效果，但仍有许多值得研究和深入的地方：

（1）文本构图方式仅仅采用滑动窗口是不足的，相邻的两个单词之间不一定具有高相关的联系。需要引入其他的构图方式，确保单词之间的边是有意义的。例如语法依赖解析树的方式构建图是一种方式。但是并不是
适用于所有模型，如语法解析树构图方式在本文提出的方法上效果并不好。采用何种构图方式是提升模型性能的关键之一。

（2）

（3）当引入BERT预训练模型后，算法性能能到很大提升。这是由于训练样本较少，模型难以完全拟合所有情况，因而一个经过大量语料库的预训练的模型有助于提升下游任务的性能。针对某一任务的
训练样本是有限的，因此，一个通用的预训练模型是值得研究的。


